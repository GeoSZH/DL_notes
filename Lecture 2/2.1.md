## When gradient is small
### 为什么Optimization会失败？
#### gradient is close to zero
当梯度接近0甚于等于0的时候，参数就会更新很慢或者停止更新，但是这些点并不是一定是**local minima** 也有可能是**saddle point**（如图所示），这些我们都统称为**critical point**。

![image](https://user-images.githubusercontent.com/88269254/170284495-d8c363ee-1b47-424a-af1c-bf6a25fdc001.png)

当我们遇到gradient是0的时候，我们要先判断以下到底是local minima还是saddle point，因为如果是local minima，那确实没有办法更新了，而处在saddle point，我们是由更低的点，我们是有可能改善这种状况的。

### local minima or saddle point？
想要判断critical point的类型需要知道loss function的形状，但是在实际情况中loss function往往非常复杂，我们没有办法得到它的具体形状，但这并不是说我们就无法判断了，我们可以某一组参数附近的loss function，也就是很局部的一小段，可以用泰勒多项式逼近，如下图：
![image](https://user-images.githubusercontent.com/88269254/170286432-92c85a00-1467-4e1c-b04b-7d2c01422fbd.png)

其中g代表了gradient，H则代表了Hessian矩阵，由于我们是处在critical point，所以g=0，我们可以直接忽略第二项，把式子简写。而简写后的式子就给了我们判断critical point的证据：

![image](https://user-images.githubusercontent.com/88269254/170287555-0fadc025-eab9-41ec-ac8a-e55c7dfbac08.png)

就像图中所示，我们可以根据第三项的值分为三种情况，一种情况是第三项大于0，那就证明在θ

![image](https://user-images.githubusercontent.com/88269254/170287764-fbb6260c-5303-422a-bbea-9c6f1b202b23.png)
