## Classification
### One-hot vector
为什么不直接用数字1，2，3？有一个很容易发现的点就是，我们用1，2，3的话1和3之间的距离比1和2似乎远一些，证明他们更不相关吗？那肯定不是，在实际情况中，我们很可能需要分的类之间互不相关，而采用one-hot则不会有这个问题，因为每个向量之间的距离都是一样。

![image](https://user-images.githubusercontent.com/88269254/170867244-8a50c4cd-56be-4f23-8f99-f40ed73ac41c.png)

### 为什么是softmax？


### softmax是什么？
![image](https://user-images.githubusercontent.com/88269254/170870760-dda53622-c167-4c5b-9223-27181891b01f.png)

softmax在两个类别与sigmoid其实是等价的。推导可查阅网上资料。MSE与maximizing likelihood也是一样的，这个可见logistic regression的推导。

### 需要注意
如果在用cross-entropy的时候pytorch或者tensorflow会自动的给神经网络的最后一层加上一层softmax函数，所以无需再手动添加softmax。


### Maximum Likelihood
什么是最大似然？李宏毅老师在课上举了一个例子，就是我们目前得到的79个样本可能是任意一个高斯分布Gaussian Distribution产生的，只是有一些高斯分布产生我们已经得到的79个样本的概率很低，而另一些概率较高，而最大似然就是高斯分布产生79个样本点的概率，因为是独立采样，所以为79个概率相乘即可得到。

